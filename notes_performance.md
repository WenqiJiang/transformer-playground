# Microbenchmarks

## BERT base (110M)

Model: https://huggingface.co/bert-base-uncased

A blog about BERT performance: 
https://www.lettria.com/articles/an-empirical-approach-to-speedup-your-bert-inference-with-onnx-torchscript#:~:text=Inference%20time%20ranges%20from%20around,depending%20on%20the%20hardware%20setup.

Hardware: RTX 3090

Sequence length will influence inference performance

Batch size = 1, length = 1 

time consumption: 7.851600646972656 ms
time consumption: 7.208347320556641 ms
time consumption: 7.092475891113281 ms

Batch size = 1, length = 512

time consumption: 10.63227653503418 ms
time consumption: 11.115312576293945 ms
time consumption: 10.342836380004883 ms


Batch size = 16, length = 512

time consumption: 25.914907455444336 ms
time consumption: 24.76668357849121 ms
time consumption: 25.20275115966797 ms

Batch size = 32, length = 512

time consumption: 29.204607009887695 ms
time consumption: 28.952598571777344 ms
time consumption: 29.051780700683594 ms

Batch size = 64, length = 512 -> OOM

## GPT

Run with HuggingFace GPT (124 M)

max length = 512

Only computes the state (all input tokens fixed)

need to warmup the model (run a single pass of inference)

Batch size = 1

len=1
shape last_hidden_states torch.Size([1, 1, 768])
time consumption: 0.0065860748291015625 sec

len=128
shape last_hidden_states torch.Size([1, 128, 768])
time consumption: 0.010244369506835938 sec

len=512
shape last_hidden_states torch.Size([1, 512, 768])
time consumption: 0.01290273666381836 sec

0.025 ms = 25 us per time step


Batch size = 32

shape last_hidden_states torch.Size([32, 512, 768])
time consumption: 0.029543399810791016 sec


Batch size = 64/128 error

model: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf 
768-dim state, k, v float, 512 length, 12 layers = 4 * 2 * 768 * 512 * 12 = 37,748,736 = 38 MB per sequence

torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 23.69 GiB total capacity; 21.73 GiB already allocated; 528.69 MiB free; 22.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

### Run token prediction with third-party PyTorch GPT2

With a starting sequence, generate the rest with beam, implemented by some github user: https://github.com/graykode/gpt-2-Pytorch 

Generating 512 tokens took 5.410696744918823 sec

Highly suspect itâ€™s because this implementation itself is not efficient

## GPT2 medium (355M)

Model: https://huggingface.co/gpt2 

Max seq length = 1024
Dim = 1024
Layer = 24

Once the model is copied to GPU (without any addtional input), it consumes 2093 MB of memory, while the model only has 355 M parameters. The same phenomenon happens on GPT2 small (124 M params, 1255 MB GPU memory usage).

Batch size = 1, Len = 512

shape last_hidden_states torch.Size([1, 512, 1024])
time consumption: 0.03490328788757324 sec

Batch size = 2, Len = 512

shape last_hidden_states torch.Size([2, 512, 1024])
time consumption: 0.059242963790893555 sec

Batch size = 4, Len = 512

shape last_hidden_states torch.Size([4, 512, 1024])
time consumption: 0.10682034492492676 sec

Batch size = 8, Len = 512

shape last_hidden_states torch.Size([8, 512, 1024])
time consumption: 0.1989736557006836 sec

Batch size = 16, Len = 512 -> fail


Batch size = 1, Len = 1024

shape last_hidden_states torch.Size([1, 1024, 1024])
time consumption: 69.21124458312988 ms (67.65517554558151 us per step)

Batch size = 2, Len = 1024

shape last_hidden_states torch.Size([2, 1024, 1024])
time consumption: 129.58574295043945 ms (126.6722804989633 us per step)

Batch size = 4, Len = 1024

shape last_hidden_states torch.Size([4, 1024, 1024])
time consumption: 241.26744270324707 ms (235.8430524958427 us per step)

Batch size = 8, Len = 1024 -> fail


