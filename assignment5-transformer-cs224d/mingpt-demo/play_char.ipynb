{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 254 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('../wiki.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/04/2023 16:23:21 - INFO - mingpt.model -   number of parameters: 2.554573e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 816: train loss 0.54068. lr 3.000451e-04: 100%|██████████████████████████████████████████████████████████████████████████████| 817/817 [03:03<00:00,  4.46it/s]\n",
      "epoch 2 iter 816: train loss 0.22175. lr 6.000000e-05: 100%|██████████████████████████████████████████████████████████████████████████████| 817/817 [03:04<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Wenqi: by default will use all GPUs to run -> num_workers=4 as in the TrainerConfig\n",
    "\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! .\n",
      "Lynne Codway. Born in Toronto, Ontario, Canada of Italian descent .\n",
      "Georges Hanna Sabbagh. Georges Hanna Sabbagh was born at Alexandria in Egypt .\n",
      "David Lister. David Lister (born 1930, Grimsby, Lincolnshire) is an eminent British origami historian .\n",
      "Ely Devons. Ely Devons (29 July 1913 -- 28 December 1967), an economist and statistician, was born in Bangor, Gwynedd North Wales, lived most of his life in Dublin .\n",
      "John Cobbett. John Cobbett is a sculptor born in Edinburgh in 1929 .\n",
      "Brian Gallon. Brian Gallon (born 1972 Jerusalem) is a German video artist .\n",
      "Robert Holden. Robert Holden is a British landscape architect born in Preston and educated at the University of Edinburgh .\n",
      "Lew Johnston. Lew Johnston (born 1955) is a British-South African journalist and historian .\n",
      "Benjamin Pine. Born in 1809 in Denmark, Dr Own grew up in the suburban town of Wallington .\n",
      "Gritakumar E. Chitty. Gritakumar E Chitty (b. 14 June 1939, Colombo) is a Sri Lankan Jurists and former Registrar of the International Tribunal for the Law of the Sea .\n",
      "Des Abbott. Des Abbott (born 10 January 1988 in Darwin, Northern Territory) is a field hockey forward from Australia, who won the bronze medal at the 2000 Summer Olympics .\n",
      "Peter Dowdeswell. English gourmand Peter Dowdeswell, born in London on 29 July 1947 in Sweden, is one of two guitarists, one of the founding members and the main songwriter of the Swedish death metal band Dark Tranquillity .\n",
      "John Chester Buttre. John Chester Buttre (10 June 1816 Auburn, New York - 2 December 1893 Paris) was a French Roman Catholic thinker and politician, who in 1894 founded le Sillon (``The Furrow''), a liberal Catholic movement .\n",
      "Leslie Allen. Leslie Allen (5 February 1904 Chicago, Illinois -- 1 May 1977 Wilmette, Illinois) was an American racecar driver .\n",
      "Roy Sekoff. Roy Sekoff is the founding editor of The Huffington Post .\n",
      "Samuel Rickard Christophers British Championships, Charlop was born in 1877 in France and died 192 was a French Jesuit archaeologist\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A little cat foundated in Great Britain, in 1825 and is one of the worlds leading television actress .\n",
      "Michael Finney. Michael Finney is a professional magician .\n",
      "Ian Hancock. Ian Hancock (Romani: Yanko le Redžosko) (born August 29, 1942) is a linguist, Romani scholar, and political advocate .\n",
      "Graham Roberts. Graham Roberts (October 10, 1929--October 28, 2005) was an American psychologist born in Paris .\n",
      "Nick Philip. Nick Philip (b. 1968 in London) is a graphic and multi-media artist and clothing designer operating out of the San Francisco Bay area .\n",
      "Brendan Guilfoyle. Brendan Guilfoyle born 16 July 1984 in Kilkenny, Ireland is rugby league player for the Treaty City Titans in the Irish Elite League .\n",
      "Han Zhidong. Han Zhidong (born 1975 Istanbul) is a German performance artist, animateur and politician .\n",
      "Raymond R. Schumacher. Born in Chicago, Raymond R Schumacher attended Tilden Technical High School, studying engineering, and was awarded the Purdue Club's 1942 Kizer MVP Award in football .\n",
      "Pierre Sabatie. Pierre Sabatie born in France is a rugby league player for the Villeneuve Leopards in the Elite One Championship .\n",
      "Nariman Farvardin. The company shared the same name as a mountain range in Iran where Farvardin was born .\n",
      "Jorn Madslien. Born in 1961 in Oslo, Norway, Jorn was a medical officer in the Norwegian cavalry, before reading philosophy at the University of Oslo and economics at the University of Leeds .\n",
      "Herbert Kingsford. Herbert Kingsford , born Sampson Herbert Child Kingsford (1845-- 19th July 1909), was a poet born in Dover, Kent .\n",
      "Frank Sayers. Born in London on 3 March 1763, being baptised at St Margaret Pattens on 3 April, he was son of Francis Sayers, an insurance broker, by his wife Anne, daughter of John Morris of Great Yarmouth .\n",
      "Thanhha Lai. Thanhhha was born in Texas, Granha is a biokeer and diaght his sculptor born in Edinburgh .\n",
      "David Basnett. Born in Liverpool, Basnett studied at Quarry Bank High School before becoming a bank clerk .\n",
      "James Hamilton. Bor\n"
     ]
    }
   ],
   "source": [
    "# well that was fun\n",
    "context = \"A little cat \"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
